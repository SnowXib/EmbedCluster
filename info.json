{
    "info_main_screen": "1. t-SNE (t-Distributed Stochastic Neighbor Embedding)\n      t-SNE — это техника визуализации данных, которая уменьшает количество измерений \n       данных для их удобного представления в двух- или трёхмерном пространстве.\n      \n      Основные особенности:\n      Использование вероятностей: t-SNE пытается минимизировать разницу между \n        расстояний между точками в высокомерном пространстве и в пространстве меньших размерностей.\n      Локальные структуры: Алгоритм хорошо сохраняет локальные структуры данных, то есть \n        группирует похожие точки рядом друг с другом.\n      Высокая вычислительная сложность: Подходит для относительно небольших наборов \n       (до 10 000 точек).\n      Основные шаги: t-SNE преобразует евклидовы расстояния между точками в вероятности, \n        отражают сходство точек друг с другом. После этого он находит проекцию в меньшем \n        пространстве, которая минимизирует расхождение этих вероятностей.\n      Преимущества:\n      Эффективно для визуализации сложных многомерных данных.\n      Хорошо выделяет кластеры.\n      Недостатки:\n      Не очень хорошо сохраняет глобальные структуры данных.\n      Зависимость от гиперпараметров (например, perplexity).\n      Результат зависит от начальных условий, что может приводить к разным визуализациям для\n        одного и того же набора данных.\n      2. UMAP (Uniform Manifold Approximation and Projection)\n      UMAP — это алгоритм уменьшения размерности, схожий с t-SNE, но с более низкой\n        вычислительной сложностью и более точным сохранением глобальных структур данных.\n      \n      Основные особенности:\n      Геометрическая интерпретация: UMAP использует теорию графов и топологию для\n        построения низкоразмерного представления данных,\n        основываясь на идее локальной взаимосвязанности.\n      Быстрее t-SNE: Значительно быстрее t-SNE и масштабируется на больший объём данных.\n      Сохранение глобальной структуры: Лучше сохраняет глобальную структуру данных по\n        сравнению с t-SNE.\n      Параметры: Имеет два ключевых параметра — n_neighbors, который контролирует\n       локальность структуры, и min_dist, который управляет расстоянием между точками в проекции.\n      Преимущества:\n      Высокая производительность и масштабируемость.\n      Хорошо подходит как для визуализации, так и для использования в реальных\n        задачах кластеризации и классификации.\n      Недостатки:\n      Как и t-SNE, результат сильно зависит от выбора параметров.\n      Может быть сложнее интерпретировать визуализированные результаты.\n      3. Truncated SVD (Truncated Singular Value Decomposition)\n      Truncated SVD — это алгоритм, основанный на методе сингулярного разложения, который\n        сокращает размерность данных.\n        Часто используется для разложения разреженных матриц, например, в\n        задачах обработки текстов.\n      \n      Основные особенности:\n      Похожа на PCA: Truncated SVD выполняет те же операции, что и PCA, но без вычитания\n        среднего значения (центрирования данных),\n        что делает её подходящей для работы с разреженными матрицами.\n      Часто используется в NLP: Алгоритм хорошо работает с матрицами документ-термин\n        в задачах обработки естественного языка (например, латентно-семантический анализ).\n      Сингулярные значения: Truncated SVD сокращает количество измерений, оставляя только\n        наиболее значимые сингулярные значения и соответствующие векторы.\n      Преимущества:\n      Эффективен для работы с большими разреженными матрицами.\n      Подходит для задач текстовой аналитики, где PCA не может применяться напрямую.\n      Недостатки:\n      Может терять часть информации при сильной редукции размерности.\n      Не подходит для сильно коррелированных данных.\n      4. ICA (Independent Component Analysis)\n      ICA — это метод уменьшения размерности, который используется для выделения\n        статистически независимых компонентов из данных.\n      \n      Основные особенности:\n      Независимость компонентов: В отличие от PCA, который ищет ортогональные компоненты,\n        ICA стремится найти компоненты, которые статистически независимы.\n      Применение: ICA широко используется в задачах, таких как разделение источников\n        (например, разделение смешанных аудио-сигналов).\n      Модель генерации: Алгоритм предполагает, что наблюдаемые данные — это линейная\n        комбинация независимых источников.\n      Преимущества:\n      Эффективен для задач, где важно выявить независимые сигналы или источники данных\n        (например, задачи звукового и визуального разделения).\n      Применим в биоинформатике и анализе временных рядов.\n      Недостатки:\n      Меньшая интерпретируемость по сравнению с PCA и SVD.\n      Может быть чувствителен к шуму и аномалиям в данных.\n      5. MDS (Multidimensional Scaling)\n      MDS — это метод уменьшения размерности, который проецирует данные из\n        многомерного пространства в пространство меньшей размерности,\n        сохраняя при этом расстояния между точками\n        как можно ближе к исходным.\n      \n      Основные особенности:\n      Сохранение расстояний: MDS стремится сохранить относительные расстояния между точками\n        данных в высокоразмерном пространстве\n        при проекции в пространство с меньшим количеством измерений.\n      Глобальная структура: Хорошо сохраняет глобальную структуру данных по сравнению с методами,\n        такими как t-SNE.\n      Подходит для любой метрики: MDS можно использовать с различными метриками\n        расстояний (например, евклидово расстояние или расстояние Манхэттена).\n      Преимущества:\n      Сохраняет глобальные структуры и взаимные расстояния между точками.\n      Легко интерпретируется.\n      Недостатки:\n      Медленно работает на больших наборах данных.\n      Часто может терять локальные структуры данных.\n      6. PCA (Principal Component Analysis)\n      PCA — это один из самых распространённых методов уменьшения размерности, который\n        используется для выявления главных компонент (осей наибольшей дисперсии) в данных.\n      \n      Основные особенности:\n      Поиск главных компонент: PCA находит ортогональные оси в данных, вдоль которых\n        наблюдается наибольшая дисперсия.\n      Линейная модель: PCA является линейным методом и хорошо работает с данными, которые\n        имеют линейные зависимости между признаками.\n      Применение: Широко используется в машинном обучении для предварительной обработки\n        данных, визуализации и уменьшения размерности.\n      Преимущества:\n      Простота реализации и высокая эффективность.\n      Сохраняет максимум информации при уменьшении размерности.\n      Недостатки:\n      Теряет значимую информацию, если данные нелинейны.\n      Требует центрирования данных, что может быть проблематично для разреженных матриц."
}